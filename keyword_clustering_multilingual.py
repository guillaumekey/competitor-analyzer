import streamlit as st
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
import plotly.express as px
from collections import Counter
import spacy
import os
import warnings

# Configuration pour les mod√®les de langue
LANGUAGE_MODELS = {
    'fr': 'fr_core_news_sm',
    'en': 'en_core_web_sm',
    'it': 'it_core_news_sm',
    'de': 'de_core_news_sm',
    'es': 'es_core_news_sm',
    'pt': 'pt_core_news_sm'
}

# Dictionnaire des stopwords pour diff√©rentes langues
STOPWORDS = {}


# Fonction pour charger ou t√©l√©charger des mod√®les de langue
@st.cache_resource
def load_language_model(lang_code):
    """
    Charge un mod√®le de langue spaCy. Le t√©l√©charge s'il n'est pas disponible.

    Args:
        lang_code: Code de langue (fr, en, it, de, es, pt)

    Returns:
        Mod√®le spaCy charg√©
    """
    model_name = LANGUAGE_MODELS.get(lang_code)
    if not model_name:
        st.warning(f"Mod√®le pour la langue '{lang_code}' non configur√©. Utilisation du mod√®le anglais par d√©faut.")
        model_name = 'en_core_web_sm'

    try:
        # Essayer de charger le mod√®le
        return spacy.load(model_name)
    except OSError:
        # Si le mod√®le n'est pas disponible, le t√©l√©charger
        with st.spinner(f"T√©l√©chargement du mod√®le de langue {model_name}..."):
            try:
                spacy.cli.download(model_name)
                return spacy.load(model_name)
            except Exception as e:
                st.error(f"Impossible de t√©l√©charger le mod√®le {model_name}: {str(e)}")
                # Fallback sur le mod√®le anglais qui est g√©n√©ralement disponible
                try:
                    spacy.cli.download("en_core_web_sm")
                    return spacy.load("en_core_web_sm")
                except:
                    st.error(
                        "Impossible de charger un mod√®le de langue. Veuillez installer manuellement spaCy et ses mod√®les.")
                    return None


# Fonction pour d√©tecter automatiquement la langue
def detect_language(text):
    """
    D√©tecte la langue d'un texte.

    Args:
        text: Texte √† analyser

    Returns:
        Code de langue (fr, en, it, de, es, pt) ou 'en' par d√©faut
    """
    try:
        from langdetect import detect
        lang = detect(text)
        if lang in LANGUAGE_MODELS:
            return lang
        return 'en'  # Fallback sur l'anglais
    except:
        # Si langdetect n'est pas disponible ou √©choue
        return 'en'  # Fallback sur l'anglais


# Fonction pour charger les stopwords pour toutes les langues support√©es
@st.cache_data
def load_stopwords():
    """Charge les stopwords NLTK pour toutes les langues support√©es."""
    global STOPWORDS

    # Mapping des langues NLTK
    nltk_langs = {
        'fr': 'french',
        'en': 'english',
        'it': 'italian',
        'de': 'german',
        'es': 'spanish',
        'pt': 'portuguese'
    }

    # T√©l√©charger les stopwords si n√©cessaire
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        nltk.download('stopwords')

    # Charger tous les stopwords disponibles
    for lang_code, nltk_lang in nltk_langs.items():
        try:
            STOPWORDS[lang_code] = set(stopwords.words(nltk_lang))
        except:
            # Si la langue n'est pas disponible, utiliser un ensemble vide
            STOPWORDS[lang_code] = set()

    return STOPWORDS


# Charger les stopwords au d√©marrage
load_stopwords()


# Mod√®le multilingue pour les embeddings
@st.cache_resource
def load_multilingual_model():
    """Charge un mod√®le multilingue pour les embeddings."""
    try:
        # Essayer de charger le mod√®le multilingue si disponible
        return spacy.load("xx_ent_wiki_sm")
    except:
        try:
            # Sinon, t√©l√©charger et retourner
            spacy.cli.download("xx_ent_wiki_sm")
            return spacy.load("xx_ent_wiki_sm")
        except:
            # Si √ßa √©choue, retourner None
            return None


# T√©l√©charger les ressources NLTK n√©cessaires
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')


def preprocess_keywords(df, text_column='Mot cl√©'):
    """
    Pr√©traitement des mots-cl√©s pour le clustering.

    Args:
        df: DataFrame avec les mots-cl√©s
        text_column: Nom de la colonne contenant les mots-cl√©s

    Returns:
        DataFrame avec les mots-cl√©s pr√©trait√©s
    """
    # Cr√©er une copie pour √©viter de modifier l'original
    processed_df = df.copy()

    # Normaliser les mots-cl√©s (minuscules, sans accents)
    processed_df['processed_keyword'] = processed_df[text_column].str.lower()

    # D√©tecter la langue de chaque mot-cl√©
    if 'detected_language' not in processed_df.columns:
        # Appliquer la d√©tection de langue sur un √©chantillon pour r√©duire le temps de traitement
        # Pour les petits datasets, analyser tous les mots-cl√©s
        if len(processed_df) <= 1000:
            processed_df['detected_language'] = processed_df[text_column].apply(
                lambda x: detect_language(x) if isinstance(x, str) and x.strip() else 'en'
            )
        else:
            # Pour les grands datasets, faire un √©chantillon et attribuer la langue majoritaire
            # Cela fonctionne bien pour les jeux de donn√©es unilingues
            sample_size = min(1000, len(processed_df) // 10)
            sample = processed_df[text_column].dropna().sample(sample_size)
            langs = [detect_language(text) for text in sample if isinstance(text, str) and text.strip()]

            if langs:
                # Langue majoritaire
                dominant_lang = max(set(langs), key=langs.count)
                processed_df['detected_language'] = dominant_lang
            else:
                # Fallback sur l'anglais
                processed_df['detected_language'] = 'en'

    # Supprimer les caract√®res sp√©ciaux, mais conserver les espaces
    processed_df['processed_keyword'] = processed_df['processed_keyword'].apply(
        lambda x: re.sub(r'[^\w\s]', '', x) if isinstance(x, str) else x
    )

    return processed_df


def create_keyword_vectors(processed_df, method='tfidf', max_features=5000):
    """
    Cr√©e des vecteurs √† partir des mots-cl√©s selon diff√©rentes m√©thodes.

    Args:
        processed_df: DataFrame avec les mots-cl√©s pr√©trait√©s
        method: M√©thode de vectorisation ('tfidf', 'spacy', 'multilingual')
        max_features: Nombre maximum de caract√©ristiques pour TF-IDF

    Returns:
        Matrice de caract√©ristiques et vectorizer (si applicable)
    """
    # S'assurer que la colonne existe
    if 'processed_keyword' not in processed_df.columns:
        st.error("La colonne 'processed_keyword' n'existe pas. Ex√©cutez d'abord le pr√©traitement.")
        return None, None

    # Supprimer les valeurs NaN
    keywords = processed_df['processed_keyword'].dropna().tolist()

    if method == 'tfidf':
        # Vectorisation TF-IDF adapt√©e multilingue
        # Collecter tous les stopwords de toutes les langues
        all_stopwords = set()
        for lang_stopwords in STOPWORDS.values():
            all_stopwords.update(lang_stopwords)

        vectorizer = TfidfVectorizer(
            max_features=max_features,
            stop_words=list(all_stopwords),
            ngram_range=(1, 2)  # Unigrammes et bigrammes
        )
        feature_matrix = vectorizer.fit_transform(keywords)
        return feature_matrix, vectorizer

    elif method == 'spacy':
        # Vectorisation avec spaCy par langue
        vectors = []

        # Obtenir la langue de chaque mot-cl√©
        languages = processed_df['detected_language'].dropna().tolist()

        # S'assurer que nous avons une langue pour chaque mot-cl√©
        if len(languages) != len(keywords):
            # Utiliser l'anglais comme fallback
            languages = ['en'] * len(keywords)

        # Charger tous les mod√®les n√©cessaires
        needed_models = set(languages)
        loaded_models = {}

        for lang in needed_models:
            model = load_language_model(lang)
            if model:
                loaded_models[lang] = model

        # Traiter chaque mot-cl√© avec le mod√®le de sa langue
        for i, keyword in enumerate(keywords):
            lang = languages[i] if i < len(languages) else 'en'
            model = loaded_models.get(lang) or loaded_models.get('en')

            if model and keyword:
                doc = model(keyword)
                if doc.vector.any():  # V√©rifier que le vecteur n'est pas nul
                    vectors.append(doc.vector)
                else:
                    # Fallback si le mot est inconnu: vecteur de z√©ros
                    vectors.append(np.zeros(model.vocab.vectors.shape[1]))
            else:
                # Si pas de mod√®le disponible, cr√©er un vecteur vide
                # Utiliser une dimension standard pour spaCy
                vectors.append(np.zeros(300))

        feature_matrix = np.array(vectors)
        return feature_matrix, None

    elif method == 'multilingual':
        # Utiliser un mod√®le multilingue unique
        multilingual_model = load_multilingual_model()

        if multilingual_model:
            vectors = []
            for keyword in keywords:
                if isinstance(keyword, str) and keyword.strip():
                    doc = multilingual_model(keyword)
                    if doc.vector.any():
                        vectors.append(doc.vector)
                    else:
                        vectors.append(np.zeros(multilingual_model.vocab.vectors.shape[1]))
                else:
                    vectors.append(np.zeros(multilingual_model.vocab.vectors.shape[1]))

            feature_matrix = np.array(vectors)
            return feature_matrix, None
        else:
            st.warning("Mod√®le multilingue non disponible. Utilisation de TF-IDF √† la place.")
            return create_keyword_vectors(processed_df, method='tfidf', max_features=max_features)

    else:
        st.error(f"M√©thode de vectorisation '{method}' non reconnue")
        return None, None


def cluster_keywords(feature_matrix, method='kmeans', n_clusters=None):
    """
    Clusterise les mots-cl√©s selon diff√©rentes m√©thodes.

    Args:
        feature_matrix: Matrice de caract√©ristiques
        method: M√©thode de clustering ('kmeans', 'dbscan')
        n_clusters: Nombre de clusters (pour kmeans)

    Returns:
        Liste des labels de cluster pour chaque mot-cl√©
    """
    if feature_matrix is None or feature_matrix.shape[0] < 2:
        st.error("Donn√©es insuffisantes pour le clustering")
        return []

    # D√©terminer le nombre optimal de clusters si non sp√©cifi√©
    if method == 'kmeans' and n_clusters is None:
        # Utiliser la m√©thode du coude
        inertia = []
        silhouette_scores = []
        k_range = range(2, min(20, feature_matrix.shape[0] - 1))

        for k in k_range:
            try:
                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                kmeans.fit(feature_matrix)
                inertia.append(kmeans.inertia_)

                # Calculer le score de silhouette si possible
                if len(set(kmeans.labels_)) > 1:  # Au moins 2 clusters
                    silhouette_avg = silhouette_score(feature_matrix, kmeans.labels_)
                    silhouette_scores.append(silhouette_avg)
                else:
                    silhouette_scores.append(0)
            except Exception as e:
                warnings.warn(f"Erreur lors du calcul pour k={k}: {str(e)}")
                continue

        # Trouver le "coude" dans le graphique d'inertie ou le meilleur score de silhouette
        if silhouette_scores:
            best_k = k_range[silhouette_scores.index(max(silhouette_scores))]
        else:
            best_k = 5  # Valeur par d√©faut si la m√©thode √©choue

        n_clusters = best_k

    # Appliquer l'algorithme de clustering
    if method == 'kmeans':
        # K-means est simple et efficace pour la plupart des cas
        n_clusters = min(n_clusters, feature_matrix.shape[0] - 1)
        clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        return clusterer.fit_predict(feature_matrix)

    elif method == 'dbscan':
        # DBSCAN est utile si vous ne savez pas combien de clusters vous voulez
        clusterer = DBSCAN(eps=0.5, min_samples=5)
        return clusterer.fit_predict(feature_matrix)

    else:
        st.error(f"M√©thode de clustering '{method}' non reconnue")
        return []


def label_clusters(df, clusters, vectorizer=None, text_column='processed_keyword'):
    """
    √âtiquette automatiquement les clusters en fonction des mots les plus importants.

    Args:
        df: DataFrame avec les mots-cl√©s
        clusters: Liste des labels de cluster
        vectorizer: Vectorizer TF-IDF (si disponible)
        text_column: Nom de la colonne contenant les mots-cl√©s

    Returns:
        DataFrame avec les clusters √©tiquet√©s
    """
    if len(clusters) != len(df):
        st.error("Nombre de clusters incompatible avec le nombre de mots-cl√©s")
        return df

    # Ajouter les labels de cluster au DataFrame
    df_with_clusters = df.copy()
    df_with_clusters['cluster'] = clusters

    # G√©n√©rer des √©tiquettes pour chaque cluster
    cluster_labels = {}

    for cluster_id in set(clusters):
        if cluster_id == -1:  # -1 est utilis√© par DBSCAN pour les outliers
            cluster_labels[cluster_id] = "Outliers"
            continue

        # Extraire les mots-cl√©s de ce cluster
        cluster_keywords = df_with_clusters[df_with_clusters['cluster'] == cluster_id][text_column].tolist()

        if not cluster_keywords:
            cluster_labels[cluster_id] = f"Cluster {cluster_id}"
            continue

        # M√©thode 1: Utiliser les mots les plus fr√©quents
        all_words = ' '.join([str(kw) for kw in cluster_keywords if isinstance(kw, str)]).split()
        word_counts = Counter(all_words)
        common_words = [word for word, count in word_counts.most_common(3)]

        if common_words:
            cluster_labels[cluster_id] = f"Cluster {cluster_id}: {', '.join(common_words)}"
        else:
            cluster_labels[cluster_id] = f"Cluster {cluster_id}"

    # Ajouter les √©tiquettes au DataFrame
    df_with_clusters['cluster_label'] = df_with_clusters['cluster'].map(cluster_labels)

    return df_with_clusters


def visualize_clusters(df, feature_matrix):
    """
    Visualise les clusters de mots-cl√©s en 2D ou 3D.

    Args:
        df: DataFrame avec les clusters
        feature_matrix: Matrice de caract√©ristiques

    Returns:
        Figure Plotly
    """
    # R√©duire la dimensionnalit√© pour la visualisation
    if feature_matrix.shape[1] > 3:
        # Utiliser PCA pour r√©duire √† 3 dimensions
        pca = PCA(n_components=3)
        reduced_features = pca.fit_transform(
            feature_matrix.toarray() if hasattr(feature_matrix, 'toarray') else feature_matrix)
    else:
        reduced_features = feature_matrix

    # Cr√©er un DataFrame pour Plotly
    vis_df = pd.DataFrame()
    vis_df['x'] = reduced_features[:, 0]
    vis_df['y'] = reduced_features[:, 1]
    if reduced_features.shape[1] > 2:
        vis_df['z'] = reduced_features[:, 2]

    vis_df['mot_cle'] = df['Mot cl√©'].values[:len(reduced_features)]
    vis_df['cluster'] = df['cluster_label'].values[:len(reduced_features)]
    vis_df['volume'] = df['Volume de recherche'].values[:len(reduced_features)]

    # Cr√©er la visualisation
    if reduced_features.shape[1] > 2:
        fig = px.scatter_3d(
            vis_df, x='x', y='y', z='z',
            color='cluster', hover_name='mot_cle',
            size='volume', size_max=20,
            title='Clusters de mots-cl√©s en 3D'
        )
    else:
        fig = px.scatter(
            vis_df, x='x', y='y',
            color='cluster', hover_name='mot_cle',
            size='volume', size_max=20,
            title='Clusters de mots-cl√©s en 2D'
        )

    fig.update_layout(height=700)
    return fig


def cluster_analysis_summary(df):
    """
    Fournit une analyse des clusters identifi√©s.

    Args:
        df: DataFrame avec les clusters

    Returns:
        DataFrame avec l'analyse des clusters
    """
    if 'cluster' not in df.columns:
        return pd.DataFrame()

    # Calculer des statistiques par cluster
    cluster_stats = df.groupby('cluster_label').agg({
        'Mot cl√©': 'count',
        'Volume de recherche': ['mean', 'sum'],
        'Difficult√©': 'mean',
        'Pr√©sent chez le client': lambda x: (x == 'Oui').mean() * 100
    }).reset_index()

    # Renommer les colonnes
    cluster_stats.columns = [
        'Cluster', 'Nombre de mots-cl√©s', 'Volume moyen',
        'Volume total', 'Difficult√© moyenne', '% Pr√©sent chez le client'
    ]

    # Trier par volume total d√©croissant
    cluster_stats = cluster_stats.sort_values('Volume total', ascending=False)

    # Arrondir les valeurs num√©riques
    cluster_stats['Volume moyen'] = cluster_stats['Volume moyen'].round(0).astype(int)
    cluster_stats['Volume total'] = cluster_stats['Volume total'].round(0).astype(int)
    cluster_stats['Difficult√© moyenne'] = cluster_stats['Difficult√© moyenne'].round(1)
    cluster_stats['% Pr√©sent chez le client'] = cluster_stats['% Pr√©sent chez le client'].round(1)

    return cluster_stats


def get_cluster_top_keywords(df, n=5):
    """
    R√©cup√®re les mots-cl√©s les plus importants de chaque cluster.

    Args:
        df: DataFrame avec les clusters
        n: Nombre de mots-cl√©s √† afficher par cluster

    Returns:
        Dict avec les clusters et leurs mots-cl√©s les plus importants
    """
    top_keywords = {}

    for cluster in df['cluster_label'].unique():
        # Filtrer les mots-cl√©s de ce cluster
        cluster_df = df[df['cluster_label'] == cluster]

        # Trier par volume de recherche
        top_kw = cluster_df.sort_values('Volume de recherche', ascending=False).head(n)

        # Stocker les r√©sultats
        top_keywords[cluster] = top_kw

    return top_keywords


def render_clustering_ui(df):
    """
    Interface utilisateur pour le clustering des mots-cl√©s.

    Args:
        df: DataFrame avec les mots-cl√©s communs
    """
    st.markdown("### üß© Clustering de mots-cl√©s")

    if df is None or df.empty:
        st.warning("Aucune donn√©e disponible pour le clustering. Veuillez d'abord analyser des mots-cl√©s.")
        return

    # Options de clustering
    with st.expander("Options de clustering", expanded=True):
        col1, col2 = st.columns(2)

        with col1:
            method = st.radio(
                "M√©thode de vectorisation",
                options=["TF-IDF (multilingual)", "SpaCy (par langue)", "SpaCy (mod√®le multilingue)"],
                index=0,
                key="vectorization_method"
            )
            if method == "TF-IDF (multilingual)":
                vectorization_method = "tfidf"
            elif method == "SpaCy (par langue)":
                vectorization_method = "spacy"
            else:
                vectorization_method = "multilingual"

        with col2:
            cluster_method = st.radio(
                "M√©thode de clustering",
                options=["K-means", "DBSCAN"],
                index=0,
                key="cluster_method"
            )
            clustering_method = "kmeans" if cluster_method == "K-means" else "dbscan"

        if clustering_method == "kmeans":
            n_clusters = st.slider(
                "Nombre de clusters",
                min_value=2,
                max_value=min(50, len(df) - 1) if len(df) > 2 else 2,
                value=min(8, len(df) - 1) if len(df) > 2 else 2,
                help="Laissez vide pour d√©terminer automatiquement le nombre optimal"
            )
        else:
            n_clusters = None

    # Bouton pour lancer le clustering
    if st.button("Lancer le clustering", type="primary"):
        with st.spinner("Clustering en cours..."):
            # 1. Pr√©traitement
            st.info("√âtape 1/4: Pr√©traitement des mots-cl√©s...")
            processed_df = preprocess_keywords(df)

            # 2. Vectorisation
            st.info("√âtape 2/4: Vectorisation des mots-cl√©s...")
            feature_matrix, vectorizer = create_keyword_vectors(
                processed_df,
                method=vectorization_method
            )

            if feature_matrix is None or feature_matrix.shape[0] < 2:
                st.error("Impossible de cr√©er des vecteurs pour les mots-cl√©s.")
                return

            # 3. Clustering
            st.info("√âtape 3/4: Clustering des mots-cl√©s...")
            cluster_labels = cluster_keywords(
                feature_matrix,
                method=clustering_method,
                n_clusters=n_clusters
            )

            # 4. √âtiquetage des clusters
            st.info("√âtape 4/4: √âtiquetage des clusters...")
            clustered_df = label_clusters(
                processed_df,
                cluster_labels,
                vectorizer
            )

            # Fusionner avec le DataFrame original
            result_df = df.copy()
            result_df['cluster'] = clustered_df['cluster'].values
            result_df['cluster_label'] = clustered_df['cluster_label'].values

            # Stocker le r√©sultat en session state
            st.session_state.clustered_keywords = result_df

            # Nombre de clusters trouv√©s
            unique_clusters = len(set(cluster_labels))
            if -1 in cluster_labels:  # DBSCAN utilise -1 pour les outliers
                unique_clusters -= 1

            st.success(f"Clustering termin√© ! {unique_clusters} clusters identifi√©s.")

    # Afficher les r√©sultats du clustering s'ils existent
    if 'clustered_keywords' in st.session_state and not st.session_state.clustered_keywords.empty:
        clustered_df = st.session_state.clustered_keywords

        # Cr√©er des onglets pour les diff√©rentes vues
        clustering_tabs = st.tabs(
            ["Aper√ßu des clusters", "Visualisation", "Tableau de donn√©es", "Mots-cl√©s par cluster"])

        with clustering_tabs[0]:
            # R√©sum√© des clusters
            st.markdown("#### üìä R√©sum√© des clusters")
            cluster_summary = cluster_analysis_summary(clustered_df)

            # Styliser le tableau
            def style_cluster_summary(df):
                return df.style.format({
                    'Volume moyen': lambda x: f"{x:,}".replace(',', ' '),
                    'Volume total': lambda x: f"{x:,}".replace(',', ' '),
                    'Difficult√© moyenne': '{:.1f}',
                    '% Pr√©sent chez le client': '{:.1f}%'
                })

            st.dataframe(
                style_cluster_summary(cluster_summary),
                use_container_width=True,
                height=400
            )

        with clustering_tabs[1]:
            # Visualisation des clusters
            st.markdown("#### üîç Visualisation des clusters")

            try:
                # R√©cup√©rer les donn√©es vectoris√©es
                processed_df = preprocess_keywords(df)
                feature_matrix, _ = create_keyword_vectors(
                    processed_df,
                    method=vectorization_method
                )

                if feature_matrix is not None:
                    fig = visualize_clusters(clustered_df, feature_matrix)
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    st.warning("Impossible de cr√©er la visualisation des clusters.")
            except Exception as e:
                st.error(f"Erreur lors de la visualisation: {str(e)}")

        with clustering_tabs[2]:
            # Tableau complet des mots-cl√©s avec leurs clusters
            st.markdown("#### üìã Tableau complet des mots-cl√©s")

            # Colonnes √† afficher
            display_cols = ['Mot cl√©', 'cluster_label', 'Volume de recherche',
                            'Difficult√©', 'Pr√©sent chez le client', 'Nombre de fichiers']
            display_df = clustered_df[display_cols].sort_values('cluster_label')

            # Renommer les colonnes
            display_df = display_df.rename(columns={'cluster_label': 'Cluster'})

            # Styliser le tableau
            def style_keyword_table(df):
                return df.style.format({
                    'Volume de recherche': lambda x: f"{int(x):,}".replace(',', ' '),
                    'Difficult√©': lambda x: f"{int(x):,}".replace(',', ' ')
                })

            st.dataframe(
                style_keyword_table(display_df),
                use_container_width=True,
                height=500
            )

        with clustering_tabs[3]:
            # Afficher les mots-cl√©s les plus importants de chaque cluster
            st.markdown("#### üîë Mots-cl√©s principaux par cluster")

            top_keywords = get_cluster_top_keywords(clustered_df, n=10)

            for cluster, keywords_df in top_keywords.items():
                with st.expander(f"{cluster} ({len(keywords_df)} mots-cl√©s)", expanded=False):
                    # Afficher les 10 premiers mots-cl√©s par volume
                    display_cols = ['Mot cl√©', 'Volume de recherche', 'Difficult√©', 'Pr√©sent chez le client']
                    st.dataframe(
                        keywords_df[display_cols],
                        use_container_width=True,
                        hide_index=True
                    )